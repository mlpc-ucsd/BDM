<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "Times New Roman", Times, serif;
        font-weight: 300;
        font-size: 17px;
        margin-left: auto;
        margin-right: auto;
        width: 980px;
    }

    h1 {
        font-weight: 300;
        line-height: 1.15em;
    }

    h2 {
        font-size: 1.75em;
    }

    a:link,
    a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    h1,
    h2,
    h3 {
        text-align: center;
    }

    h1 {
        font-size: 40px;
        font-weight: 500;
    }

    h2 {
        font-weight: 400;
        margin: 16px 0px 4px 0px;
    }

    .paper-title {
        padding: 16px 0px 16px 0px;
    }

    section {
        margin: 32px 0px 32px 0px;
        text-align: justify;
        clear: both;
    }

    .col-7 {
        width: 14.2%;
        float: left;
    }

    .col-6 {
        width: 16.6%;
        float: left;
    }

    .col-5 {
        width: 20%;
        float: left;
    }

    .col-4 {
        width: 25%;
        float: left;
    }

    .col-3 {
        width: 33%;
        float: left;
    }

    .col-2 {
        width: 50%;
        float: left;
    }

    .row,
    .author-row,
    .affil-row {
        overflow: auto;
    }

    .author-row,
    .affil-row {
        font-size: 20px;
    }

    .row {
        margin: 16px 0px 16px 0px;
    }

    .authors {
        font-size: 18px;
    }

    .affil-row {
        margin-top: 16px;
    }

    .teaser {
        max-width: 100%;
    }

    .text-center {
        text-align: center;
    }

    .screenshot {
        width: 256px;
        border: 1px solid #ddd;
    }

    .screenshot-el {
        margin-bottom: 16px;
    }

    hr {
        height: 1px;
        border: 0;
        border-top: 1px solid #ddd;
        margin: 0;
    }

    .material-icons {
        vertical-align: -6px;
    }

    p {
        line-height: 1.25em;
    }

    bold {
        font-weight: bold;
    }

    .caption {
        line-height: 1em;
        font-size: 14px;
        /*font-style: italic;*/
        color: #666;
        text-align: left;
        margin-top: 8px;
        margin-bottom: 8px;
    }

    video {
        display: block;
        margin: auto;
    }

    figure {
        display: block;
        margin: auto;
        margin-top: 10px;
        margin-bottom: 10px;
    }

    #bibtex pre {
        font-size: 13px;
        background-color: #eee;
        padding: 16px;
    }

    .blue {
        color: #2c82c9;
        font-weight: bold;
    }

    .orange {
        color: #d35400;
        font-weight: bold;
    }

    .flex-row {
        display: flex;
        flex-flow: row wrap;
        justify-content: space-around;
        padding: 0;
        margin: 0;
        list-style: none;
    }

    .paper-btn {
        position: relative;
        text-align: center;

        display: inline-block;
        margin: 8px;
        padding: 8px 8px;

        border-width: 0;
        outline: none;
        border-radius: 2px;

        background-color: #1367a7;
        color: #ecf0f1 !important;
        font-size: 20px;
        width: 100px;
        font-weight: 600;
    }

    .supp-btn {
        position: relative;
        text-align: center;

        display: inline-block;
        margin: 8px;
        padding: 8px 8px;

        border-width: 0;
        outline: none;
        border-radius: 2px;

        background-color: #1367a7;
        color: #ecf0f1 !important;
        font-size: 20px;
        width: 220px;
        font-weight: 600;
    }

    .paper-btn-parent {
        display: flex;
        justify-content: center;
        margin: 16px 0px;
    }

    .paper-btn:hover {
        opacity: 0.85;
    }

    .container {
        margin-left: auto;
        margin-right: auto;
        padding-left: 16px;
        padding-right: 16px;
    }

    .venue {
        color: #1367a7;
    }



    .topnav {
        overflow: hidden;
        background-color: #EEEEEE;
    }

    .topnav a {
        float: left;
        color: black;
        text-align: center;
        padding: 14px 16px;
        text-decoration: none;
        font-size: 16px;
    }


    .author-block {
        display: inline-block;
    }

    .red-step, .blue-step {
        color: #FF0000; /* Red color */
        font-weight: bold;
    }
    .blue-step {
        color: #0000FF; /* Blue color */
    }
    .denoising-step {
        font-style: italic;
    }

</style>


<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
    rel='stylesheet' type='text/css'>

<head>
    <title>Bayesian Diffusion Models for 3D Shape Reconstruction</title>
    <meta property="og:description" content="Bayesian Diffusion Models for 3D Shape Reconstruction" />
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-6HHDEXF452"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-6HHDEXF452');
    </script>

</head>


<body>
    <div class="container">
        <div class="paper-title">
            <h1>Bayesian Diffusion Models for 3D Shape Reconstruction</h1>
        </div>


        <div id="authors">
            <div class="author-row text-center">
                <span class="author-block">Haiyang Xu<sup>1,*</sup>,</span>
                <span class="author-block">Yu Lei<sup>2,*</sup>,</span>
                <span class="author-block">Zeyuan Chen<sup>3</sup>,</span>
                <span class="author-block">Xiang Zhang<sup>3</sup>,</span>
            </div>
            <div class="author-row text-center">
                <span class="author-block">Yue Zhao<sup>4</sup>,</span>
                <span class="author-block">Yilin Wang<sup>4</sup>,</span>
                <span class="author-block">Zhuowen Tu<sup>3,&dagger;</sup></span>
            </div>

            <div class="affil-row text-center">
                <span class="author-block text-center"><sup>1</sup>University of Science and Technology of China,</span>
                <span class="author-block text-center"><sup>2</sup>Shanghai Jiao Tong University,</span>
            </div>

            <div class="affil-row text-center">
                <span class="author-block text-center"><sup>3</sup>UC San Diego,</span>
                <span class="author-block text-center"><sup>4</sup>Tsinghua University</span>
            </div>

            <div class="affil-row text-center">
                <span class="author-block text-center"><sup>*</sup>Equal Contribution,</span>
                <span class="author-block text-center"><sup>&dagger;</sup>Corresponding Author</span>
            </div>

            <div style="clear: both">
                <div class="paper-btn-parent">
                    <a class="supp-btn" href="https://arxiv.org/abs/2403.06973">
                        <span class="material-icons"> description </span>
                        Paper
                    </a>
                    <a class="supp-btn" href="https://github.com/mlpc-ucsd/BDM">
                        <span class="material-icons"> description </span>
                        Code
                    </a>
                </div>
            </div>
        </div>



        <section id="abstract" />
        <h2>Abstract</h2>
        <hr>
        <p>We present Bayesian Diffusion Models (BDM), a prediction algorithm that performs effective Bayesian inference
            by tightly coupling the top-down (prior) information with the bottom-up (data-driven) procedure via joint
            diffusion processes. We show the effectiveness of BDM on the 3D shape reconstruction task. Compared to
            prototypical deep learning data-driven approaches trained on paired (supervised) data-labels (e.g.
            image-point clouds) datasets, our BDM brings in rich prior information from standalone labels (e.g. point
            clouds) to improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian frameworks where
            explicit prior and likelihood are required for the inference, BDM performs seamless information fusion via
            coupled diffusion processes with learned gradient computation networks. The specialty of our BDM lies in its
            capability to engage the active and effective information exchange and fusion of the top-down and bottom-up
            processes where each itself is a diffusion process. We demonstrate state-of-the-art results on both
            synthetic and real-world benchmarks for 3D shape reconstruction.
        </p>
        </section>

        <section id="method">
            <h2>Method</h2>
            <hr>
            <figure style="width: 100%;">
                <a href="assets/overview_pointcloud.png">
                    <img width="100%" src="assets/overview_pointcloud.png">
                </a>
                <p class="caption" style="margin-bottom: 1px; text-align: center;">
                    Overview of the generative process in our Bayesian Diffusion Model. In each Bayesian denoising step,
                    the prior diffusion model fuses with the reconstruction process, bringing rich prior knowledge and
                    improving the quality of the reconstructed point cloud. We illustrate our Bayesian denoising step in
                    two ways, left in the form of a flowchart and right in the form of point clouds.
                </p>
            </figure>
            <bold>Training</bold>: Two separately trained diffusion models: <span class="blue-step">conditional (reconstruction)</span> + <span class="red-step">unconditional (generative)</span>.
            <br><br>
            <bold>Inference</bold>: Take turns to forward the steps of the two diffusion models and fuse. 
            <ul>
                <li>
                    <span class="blue-step">Blue Step:</span> 
                    <span class="denoising-step">Reconstructive Denoising Step P(y<sup>t-1</sup> | y<sup>t</sup>, x)</span>
                    <ul>
                        <br>
                        <li>The normal diffusion-based 3D reconstruction model.</li>
                        <br>
                        <li>Given the pointcloud y<sup>t</sup> at timestep t and the image x, get the previous step result y<sup>t-1</sup>.</li>
                        <br>
                    </ul>
                </li>
                <li>
                    <span class="red-step">Red Step:</span> 
                    <span class="denoising-step">Generative Denoising Step P(y<sup>t-1</sup> | y<sup>t</sup>)</span>
                    <ul>
                        <br>
                        <li>The diffusion-based 3D generative model.</li>
                        <br>
                        <li>Given the pointcloud y<sup>t</sup> at timestep t, get the previous step result y<sup>t-1</sup>.</li>
                    </ul>
                </li>
            </ul>
            <figure style="text-align:center; width: 100%;">
                <a href="assets/fusion_module.png">
                    <img width="60%" src="assets/fusion_module.png">
                </a>
                <p class="caption" style="margin-bottom: 1px; text-align: center;">
                    Illustration of our proposed fusion methods: BDM-Merging and BDM-Blending. The left part is the BDM-Merging, while the right side shows the blending module.
                </p>
            </figure>
            We implement our fusion module in two ways, merging and blending. In BDM-Merging, we finetune the decoder of reconstruction model using the same data of training reconstruction model. In BDM-Blending, we choose points randomly from these two groups then forming new pointclouods.
        </section>


        <section id="results">
            <h2>Experiments</h2>
            <hr>
            <p style="text-align:center"><bold>I. Experiments on Synthetic Dataset: ShapeNet-R2N2</bold></p>
            We train both our generation model and reconstruction model using ShapeNet-R2N2. 
            <figure style="width: 90%;">
                <a href="assets/res_r2n2.png">
                    <img width="100%" src="assets/res_r2n2.png">
                </a>
                <p class="caption" style="margin-bottom: 1px; text-align: center;">
                    Quantitative results on the ShapeNet-R2N2 dataset. For each existing SOTA diffusion-based reconstruction model, we compare our two methods with it. We run our experiments on three categories: chair, airplane and car. In each category, we test under three different cases, varying the training data scale of reconstruction model. x% means that we only use x% data of a certain category in ShapeNet-R2N2 to train reconstruction model.
                </p>
            </figure>

            <figure style="width: 90%;">
                <a href="assets/vis_r2n2.png">
                    <img width="100%" src="assets/vis_r2n2.png">
                </a>
                <p class="caption" style="margin-bottom: 1px; text-align: center;">
                    Qualitative comparisons on the ShapeNet-R2N2 dataset.
                </p>
            </figure>
            <br>
            <p style="text-align:center"><bold>I. Experiments on Real-world Dataset: Pix3D</bold></p>
            To demonstrate the generalizability of prior model, we run experiments on a different setting. We train our generation model using ShapeNet-R2N2 while train reconstruction model using Pix3D, which is a real-world dataset. We run our experiments on three categories: chair, table and sofa.
            <figure style="width: 50%;">
                <a href="assets/res_pix3d.png">
                    <img width="100%" src="assets/res_pix3d.png">
                </a>
                <p class="caption" style="margin-bottom: 1px; text-align: center;">
                    Quantitative results on the Pix3D dataset.
                </p>
            </figure>

            <figure style="width: 90%;">
                <a href="assets/vis_pix3d.png">
                    <img width="100%" src="assets/vis_pix3d.png">
                </a>
                <p class="caption" style="margin-bottom: 1px; text-align: center;">
                    Qualitative comparisons on the Pix3D dataset. 
                </p>
            </figure>

        </section>


        <section id="bibtex">
            <h2>Citation</h2>
            <hr>
            <pre><code>
        @misc{xu2024bayesian,
            title={Bayesian Diffusion Models for 3D Shape Reconstruction}, 
            author={Haiyang Xu and Yu Lei and Zeyuan Chen and Xiang Zhang and Yue Zhao and Yilin Wang and Zhuowen Tu},
            year={2024},
            eprint={2403.06973},
            archivePrefix={arXiv},
            primaryClass={cs.CV}
        }
        </code></pre>
        </section>

        <br />

        <section id="paper">
            <h2>Paper</h2>
            <hr>
            <div class="flex-row">
                <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                    <a href="assets/paper.pdf"><img class="screenshot" src="assets/paper_preview.png"></a>
                </div>
                <div style="width: 50%">
                    <p><b>Bayesian Diffusion Models for 3D Shape Reconstruction</b></p>
                    <p>Haiyang Xu<sup>*</sup>, Yu Lei<sup>*</sup>, Zeyuan Chen, Xiang Zhang, Yue Zhao, Yilin Wang, Zhuowen Tu</p>

                    <div><span class="material-icons"> description </span><a
                            href="https://arxiv.org/pdf/2403.06973.pdf"> arXiv version</a></div>
                </div>
            </div>
        </section>

    </div>
</body>

</html>
